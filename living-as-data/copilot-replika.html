<!DOCTYPE html>
<html lang="en">
	<head>
	  <meta charset="UTF-8">
	  <meta name="viewport"
  content="width=device-width, initial-scale=1.0">
		<title>COPILOT VS. REPLIKA: I WALK, I FEED MY DATA.</title>
		<link rel="stylesheet" href="css/styles.css">
	</head>
	<body id="top">
		<div class="page-container">
		
		  <div class="notebook-clip clip-left"></div>
			<div class="notebook-clip clip-right"></div>
			
	 <div class="content-box">
		<header>
		  <h1>COPILOT VS. REPLIKA – I WALK, I FEED MY DATA.</h1>
		</header>
    </div>
	 
	<main>
	  <section id="description">
		<h2>6/7/8/9 Sentence, I walk feed my data. I sleep I feed my data. I eat I feed my data. I date I feed my data</h2>
	  
	  <div class="description">
		<p>Using the sentence ‘I walk, I feed my data etc,’ this film/artwork investigates how data impacts human decision-making and how chatbots like Copilot and Replika interpret this relationship. The resulting visuals and soundscapes reflect the nuances of their emotional comprehension.”</p>
	  </div>
	 </section>
	  
	  <section id="chatbot-1">
				<h2>Microsoft Copilot</h2>
				
		<div class="description">
		  <p>Copilot is a chatbot developed by Microsoft and launched on February 7, 2023. Based on a large language model, it is able to cite sources, create poems, and write songs.</p>
		</div>
	  </section>
	  
	 	  <section id="chatbot-2">
				<h2>Replika</h2>
				
		<div class="description">
		  <p>Replika is a generative AI chatbot app released in November 2017. The chatbot is trained by having the user answer a series of questions to create a specific neural network.</p>
		</div>
	  </section>
	  
	  <section id="visuals">
		<div class="section-title">
		  <div class="section-marker"></div>
		  <h2>Visual outcomes</h2>
		</div>
		
		<div class="highlighted-section">
		  <div class="subtitle">sketchbook and Touchboard</div>
		  <figure class="image-container">
			<a href="#img1">
			<img src="../images/Copilot-vs-Replika-1.jpg" alt="sketchbook."></a>
			<figcaption>Sketchbook</figcaption>
		  </figure>
		  
		  <figure class="image-container">
			<a href="#img2">
			<img src="../images/Copilot-vs-Replika-2.jpg" alt="Interactive Touch-board"></a>
			<figcaption>Interactive Touch-board.</figcaption>
		  </figure>

		  <figure class="image-container">
			<a href="#img3">
			<img src="../images/modal-gallery-3.jpg" alt="Storytellers + Machines-2024-Modal gallery"></a>
			<figcaption>Storytellers + Machines: 2024-Modal gallery.</figcaption>
		  </figure>
		</div>

		<div class="highlighted-section">
		  <div class="subtitle">About: Storytellers + Machines: 2024-Modal gallery</div>
		   <p>Our collaborative sketchbook touch board works were showcased at the Storytellers + Machines 2024 exhibition from July 2nd to 4th, 2024.
		   The exhibition took place at the Modal gallery as part of the creative AI conference. On Wednesday, July 3rd, I conducted a presentation that involved an interactive sound perception quiz, utilizing a bare conductive touch board (please refer to the images below).
		   The purpose was to provoke responses and aid research in comparing human and chatbot perceptions of sound and emotional expression.
		   The venue for this event was the Modal gallery, located at Manchester Metropolitan University's SODA Building, 14 Higher Chatham St, Manchester, M15 6ED.</p>

		   <p>The Storytellers + Machine conference was exceptionally well-organized and thoughtfully curated. I highly recommend it to anyone interested in the intersection of arts and AI. 
		   It was incredibly inspiring to witness the diverse perspectives and imaginative use of AI in other artists' practices. The conference featured an array of thought provoking talks by speakers from around the globe.
		   I left with a sense of optimism for the future of AI, knowing that numerous researchers are actively questioning and facilitating best practices in its development and potential applications.</p>
		 </div>
	  </section>
	 
	  <section id="quizz-section">
		<div class="highlighted-section">
			<div class="subtitle">"Interpreting Emotional AI Soundscapes: Audience Perception Quiz"</div>
		  
			<div class="description">
				<p>I thought it would be interesting to conduct a quiz to determine the relationship between a chatbots' understanding of emotions and that of humans. Feel free to engage in this research by clicking on the video link:"Interpreting Emotional AI Soundscapes: Audience Perception Quiz" and using the QR code presented at the start of the video to access the quiz or the link below. It is important to watch the tutorial while taking the quiz to determine which sounds correspond to each question.</p>
			</div>
		  
			<div class="content-box" style="text-align: center;">
				<a href="https://docs.google.com/forms/d/e/1FAIpQLSdKU6yN_95ePW5HaDjRFCWcpoOY19vQ-f5oQSLTsKtOA8XcPA/viewform" target="_blank" class="btn">Tap to open quizz</a>
			</div>
		</div>
	</section>
	 
	 <section id="video-1">
		<h2>How it works!: Audience Perception Quiz.</h2>
		<div class="video-container">
		   <iframe src="https://player.vimeo.com/video/960514210" frameborder="0" allow="autoplay; fullscreen; picture-in-picture" allowfullscreen title="Interpreting Emotional AI Soundscapes: Audience Perception Quiz"></iframe>
	   </div>
	 </section>
	 
	 <section id="video-2">
		<h2>visual interpretations: Sketchbook outcomes.</h2>
		<div class="video-container">
		   <iframe src="https://player.vimeo.com/video/954722268" frameborder="0" allow="autoplay; fullscreen; picture-in-picture" allowfullscreen title="visual interpretations: Sketchbook outcomes"></iframe>
	   </div>
	 </section>
	 
	   <section id="conclusion">
		<div class="section-title">
		  <div class="section-marker"></div>
		  <h2>Conclusive hypothesis: Heidi Stokes & Arunav Das</h2>
		</div>
	 
	  	<div class="highlighted-section">
		  <div class="subtitle">Comparison:  Copilot and Replica</div>

	   	<div class="description">
		 <p>During our investigation, we compared Copilot and Replica to understand their empathetic understanding. Like previous sketchbooks, I asked both companion ai’s questions focused on the analysis of a specific sentence, "I walk, I feed my data," to explore how data impacts human decision-making. This allowed me to generate prompts from both to create sound and visuals, resulting in the visual empathetic graph you see above.Please watch the video to find out the results.</p>
	  </div>
	  </div>
	 </section>
	</main>
	
				<hr class="dotted-divider">
			
			<footer>
			<p id="contact">Contact me at: <a href="mailto:heidistokes2001@yahoo.co.uk">heidistokes2001@yahoo.co.uk</a></p>
				<p><a href="#top">Back to top</a></p>
			</footer>

			<!--lightbox-->
			<div id="img1" class="lightbox">
				<a href="#">
				  <img src="../images/Copilot-vs-Replika-1.jpg" alt="sketchbook.">
				</a>
			</div>

			<div id="img2" class="lightbox">
				<a href="#">
				  <img src="../images/Copilot-vs-Replika-2.jpg" alt="sketchbook.">
				</a>
			</div>

			<div id="img3" class="lightbox">
				<a href="#">
				 <img src="../images/modal-gallery-3.jpg" alt="Storytellers + Machines-2024-Modal gallery">
				</a>
			</div>
		  
	  </div>
	</body>
</html>
