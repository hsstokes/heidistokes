<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<title>INVESTIGATING AI'S UNDERSTANDING OF EMOTIONS: DEVELOPING A MULTIMODAL GRAPH</title>
	<link rel="stylesheet" href="css/styles.css">
</head>
<body id="top">
	<div class="page-container">
		<div class="notebook-clip clip-left"></div>
		<div class="notebook-clip clip-right"></div>
		<header>
			<h1>INVESTIGATING AI'S UNDERSTANDING OF EMOTIONS: DEVELOPING A MULTIMODAL GRAPH</h1>
		</header>
		<nav>
			<ul>
				<li><a href="../index.html">About</a></li>
				<li><a href="../living-as-data.html">Living as Data</a></li>
				<li><a href="evi-empathy.html">Evi Empathy</a></li>
				<li><a href="multimodal-graph.html">Multimodal Graph</a></li>
				<li><a href="replika-study.html">Replika Study</a></li>
				<li><a href="uncanny-valley-1.html">Uncanny Valley</a></li>
				<li><a href="woebot-expanded.html">Woebot Extended</a></li>
				<li><a href="woebot-health.html">Woebot Health</a></li>
				<li><a href="tactile-thresholds.html">Tactile thresholds</a></li>
			</ul>
		</nav>
		
		<main>
			<section id="introduction">
				<div class="highlighted-section">
					<p class="subtitle">2: Sentence Analysis <b>– 'My Garmin is my body'</b></p>

					<p class="description">
						<i>"Building on the exploration of AI's emotional comprehension, this film/artwork examines how AI processes physical and emotional connections. The phrase 'My Garmin is my body' is deconstructed using hand gestures, symbols, and sound to create a multimodal representation of AI's understanding."</i>
					</p>
				</div>

				<h3>Tools used to Visually interpret AI's Emotional Comprehension</h3>

				<ol>
					<li><b>Hume AI: </b>Analysing speech prosody to detect emotions in language.</li>
					<br />
					<li><b>Poe: </b>Interpreting emotions through shapes.</li>
					<br />
					<li><b>Stable Audio: </b>Translating emotional tones into sound.</li>
					<br />
					<li><b>AI Generative Tools: </b>Exploring hand gesture interpretations through various AI models.</li>
					<br />
				</ol>
			</section>

			<section id="video">
				<figcaption>Film 2: Sentence Analysis – 'My Garmin is my body" - Building on the exploration of AI's emotional comprehension, this film examines how AI processes physical and emotional connections.</figcaption>
				<div class="video-container">
					<iframe
						src="https://player.vimeo.com/video/909469457"
						frameborder="0"
						allow="autoplay; fullscreen; picture-in-picture"
						allowfullscreen
						title="Exploring Hume.AI Process Video">
					</iframe>
				</div>
				
				<div class="content-box">
					<p><b>Technological Exploration and Challenges</b></p>
					<p>In my initial experiments, I explored merging interactive technology with visual art, using the Bare Conductive Touch Board, a projector, and electric paint to trigger animations based on physical touch.</p>
				</div>
			</section>
				
			<section id="visuals">
			  <div class="highlighted-section">
				<h2>Visual outcomes</h2>
				<figure class="image-container">
					<a href="#img1">
						<img src="../images/understanding-emotions-1.jpg" alt="Projection mapping experiment showing emotional visualization through touch sensors">
					</a>
					<figcaption>Sketchbook emotion AI visualization</figcaption>
				</figure>
				
				<figure class="image-container">
					<a href="#img2">
						<img src="../images/understanding-emotions-2.jpg" alt="Touch sensor setup with electric paint connections showing emotional response system">
					</a>
					<figcaption>Touch sensor setup with Bare Conductive board</figcaption>
				</figure>
			</div>
			  
				<div class="content-dot-box">
					<div class="section-marker"></div>
					<h3>What happened?</h3>
				</div>
				<div class="highlighted-section">
					<p>At this stage, I was really just experimenting. I was trying to develop a visual language, and I thought, "Why not use my animation skills?" That's when I discovered projection mapping, which seemed like a perfect fit.
					The idea was that, with just a click of a touch sensor, it would trigger an animation that represented a specific emotion—each gesture or symbol mimicking the emotion it was tied to.</p>

					<p>I had a basic projector at the time, and, frankly, the results weren't perfect. The concept looked okay, but the projection itself just wasn't powerful enough. It was exciting to see something happen when you clicked a button, but it didn't have the visual impact I knew I could generate with my own digital animations.</p>

					<p>However, I really liked the touch sensors. They worked well and allowed me to translate sound into visual prompts from the chatbot itself. That's when things clicked. Not only could a chatbot engage in emotional conversation with a human, but it could also interpret emotions through sound in a really interesting way.</p>

					<p>That was a pivotal moment for me—this realization that emotional communication could go beyond just language. It could be conveyed through sensory and auditory experiences as well, adding layers to how AI might truly understand and engage with emotions.</p>
				</div>
			</section>
		</main>
		
		<hr class="dotted-divider">
			
		<footer>
			<p id="contact">Contact me at: <a href="mailto:heidistokes2001@yahoo.co.uk">heidistokes2001@yahoo.co.uk</a></p>
			<p><a href="#top">Back to top</a></p>
		</footer>
			
	</div>

	<!-- Lightbox elements - place before closing body tag -->
	<a href="#" class="lightbox" id="img1">
		<img src="../images/understanding-emotions-1.jpg" alt="Projection mapping experiment showing emotional visualization through touch sensors">
	</a>
	
	<a href="#" class="lightbox" id="img2">
		<img src="../images/understanding-emotions-2.jpg" alt="Touch sensor setup with electric paint connections showing emotional response system">
	</a>

</body>
</html>
